{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPU+Hto1Pu5/mQN+qvZT3/H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saksham9804/Projects/blob/main/Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Imports"
      ],
      "metadata": {
        "id": "_AX3Fs6Q3ACz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing all the libraries I need for building, training, visualizing, and saving my neural network."
      ],
      "metadata": {
        "id": "z9Q9Ywlt3GIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pickle"
      ],
      "metadata": {
        "id": "EIk3EHdX3JJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking if a GPU is available and using it if possible—otherwise, I’ll use the CPU."
      ],
      "metadata": {
        "id": "AAnlGkrt3MP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "2t_Izgdh3Qce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "7iSBvyku3Vm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a transform to turn images into tensors and normalize them so the model can train easily."
      ],
      "metadata": {
        "id": "riSkuRL23aQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ],
      "metadata": {
        "id": "ftLPPYqa3fiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now,loading the EMNIST Balanced dataset—both the training and test splits."
      ],
      "metadata": {
        "id": "F8pWbjGp3ll0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = datasets.EMNIST(root='./data', split='balanced', train=True, download=True, transform=image_transform)\n",
        "test_set = datasets.EMNIST(root='./data', split='balanced', train=False, download=True, transform=image_transform)\n"
      ],
      "metadata": {
        "id": "1GP54pZM3n8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the training data into a training set (80%) and a validation set (20%) so I can check if my model is overfitting.\n"
      ],
      "metadata": {
        "id": "AqunPzWW3umY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(train_set))\n",
        "val_size = len(train_set) - train_size\n",
        "train_set, val_set = random_split(train_set, [train_size, val_size])"
      ],
      "metadata": {
        "id": "By-pN6X03xGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating data loaders to efficiently feed data in batches to my model.\n"
      ],
      "metadata": {
        "id": "xTPNlIC733Vm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "jafbGQt635Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition"
      ],
      "metadata": {
        "id": "zVUHnhmA4BR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I’m defining a custom neural network for the EMNIST task."
      ],
      "metadata": {
        "id": "8sJ719zo4GPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EMNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # I’m stacking convolutional layers with ReLU activations and adding batch normalization for stable learning.\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        # I’m flattening the output so it can go through fully connected layers next.\n",
        "        self.flatten = nn.Flatten()\n",
        "        # I’m adding a skip connection to help gradients flow and prevent vanishing.\n",
        "        self.skip = nn.Linear(128 * 7 * 7, 256)\n",
        "        self.fc1 = nn.Linear(128 * 7 * 7, 256)\n",
        "        self.fc2 = nn.Linear(256, 47)  # 47 possible classes in EMNIST Balanced\n",
        "\n",
        "    def forward(self, x):\n",
        "        # I’m passing data through convolutions and flattening.\n",
        "        x = self.features(x)\n",
        "        x = self.flatten(x)\n",
        "        # I’m creating a skip connection from feature output directly to fully connected output.\n",
        "        skip_out = self.skip(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = x + skip_out  # Adding the skip connection for improved learning\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cNc-RMix4IU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, creating the model and sending it to the right device (CPU or GPU) and picking cross-entropy as my loss since this is a classification problem and Adam as my optimizer.\n"
      ],
      "metadata": {
        "id": "CyQY3fTy4Pds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = EMNISTModel().to(device)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "S4Cbm4f34RfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "-34-8MG64m0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []"
      ],
      "metadata": {
        "id": "NbJMiri84yjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now looping over my specified number of epochs, training and validating each time."
      ],
      "metadata": {
        "id": "XVZvV0yf415s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    # Training phase: I’m letting the model learn from the training data.\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    train_losses.append(running_loss / len(train_loader))\n",
        "    train_accuracies.append(correct / total)\n",
        "    # Validation phase: I’m checking the model’s performance on unseen validation data.\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "    val_losses.append(val_loss / len(val_loader))\n",
        "    val_accuracies.append(val_correct / val_total)\n",
        "\n",
        "    # Printing progress after each epoch so I can see how my model is doing.\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    print(f\"  Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.4f}\")\n",
        "    print(f\"  Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.4f}\")"
      ],
      "metadata": {
        "id": "h-q6WutP43Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iaQh0-Z24-1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting Results"
      ],
      "metadata": {
        "id": "4GWNhiVi5MXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, plotting the loss and accuracy curves so I can visualize training progress."
      ],
      "metadata": {
        "id": "tvgPMBVS5O3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(val_accuracies, label='Val Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AFg6DJUD5Um-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Model and Predictions"
      ],
      "metadata": {
        "id": "Vq08_VOr5ans"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the trained model’s weights to disk using pickle."
      ],
      "metadata": {
        "id": "ppknoMgT5hiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('emnist_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model.state_dict(), f)\n",
        "print(\"Model weights saved to emnist_model.pkl\")\n"
      ],
      "metadata": {
        "id": "3joLLf925zlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Switching to evaluation mode and predicting class labels for the test set."
      ],
      "metadata": {
        "id": "-LQl8Fbz55vP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for images, _ in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predictions.extend(predicted.cpu().numpy())"
      ],
      "metadata": {
        "id": "9ptS1b_L57xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, saving the test predictions to a CSV file for submission or later use."
      ],
      "metadata": {
        "id": "01BHavYa6BAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({'id': range(len(predictions)), 'label': predictions})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Predictions saved to submission.csv\")"
      ],
      "metadata": {
        "id": "x8OG7R-p6DGa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}