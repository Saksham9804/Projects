{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5E9otlBYRoBnmHmxMKhIf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saksham9804/Projects/blob/main/RAG_Q%26A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing and importing the necessary libraries"
      ],
      "metadata": {
        "id": "EQXoCrhj7lhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing required packages\n",
        "!pip install langchain langchain-community faiss-cpu sentence-transformers transformers pypdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_7M1tvCoNx2",
        "outputId": "6892c5e7-920b-4603-8e59-dbe51de7de0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.12.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.1)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (6.0.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.14)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Modules"
      ],
      "metadata": {
        "id": "lT6GhFTVPBES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os"
      ],
      "metadata": {
        "id": "iZKOV__fO_tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading Dataset and doing the required Setups"
      ],
      "metadata": {
        "id": "N7KuCGQ_PMgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading Kaggle dataset and confirming dataset path\n",
        "path = kagglehub.dataset_download(\"sakshamtiwari98/market-analysis\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Setting the PDF directory to the downloaded path\n",
        "pdf_dir = path\n",
        "\n",
        "# Now, Listing specific PDF files to process\n",
        "\n",
        "pdf_files = [\n",
        "    \"Attention is all you need.pdf\",\n",
        "    \"BERT Pre training of Deep Bidirectional Transformers for.pdf\",\n",
        "    \"Contrastive Language-Image Pre-Training with.pdf\",\n",
        "    \"Language Models are Few-Shot Learners.pdf\",\n",
        "    \"LLaMA Open and Efficient Foundation Language Models.pdf\"\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCXxp5rEPTrk",
        "outputId": "6be3fa9b-47a8-42d4-94ac-a671678f8710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/market-analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Chunking PDF Documents into Pieces"
      ],
      "metadata": {
        "id": "tKzxrh0XPVbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_chunk_pdfs(files, dir_path, chunk_size=1000, chunk_overlap=150):\n",
        "    all_docs = []   #( Collecting all loaded document texts )\n",
        "\n",
        "    # Iterating over each file to load and extract text\n",
        "    for file in files:\n",
        "        file_path = os.path.join(dir_path, file)\n",
        "\n",
        "        # Printing warning and skipping if file does not exist(Well that will not happen :)\n",
        "        if not os.path.isfile(file_path):\n",
        "            print(f\"File not found: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        # Loading PDF content into documents\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        docs = loader.load()\n",
        "\n",
        "        # Adding loaded documents to the collection\n",
        "        all_docs.extend(docs)\n",
        "\n",
        "    # Raising error if no documents get loaded to catch issues early\n",
        "    if not all_docs:\n",
        "        raise ValueError(\"No documents were loaded. Please check your PDF files.\")\n",
        "\n",
        "    # Creating a text splitter with overlapping chunks to preserve context and splitting into smaller chunks\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    docs_split = splitter.split_documents(all_docs)\n",
        "\n",
        "    # Returning the split document chunks\n",
        "    return docs_split"
      ],
      "metadata": {
        "id": "qCm0HaQpQ24z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and chunking the PDF files"
      ],
      "metadata": {
        "id": "uKZhTMMeQ4SM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs_split = load_and_chunk_pdfs(pdf_files, pdf_dir)"
      ],
      "metadata": {
        "id": "N_nZZ7AHQ5Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing metadata like filenames or page info to prevent leakage in output"
      ],
      "metadata": {
        "id": "STUPCY3aQ-55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clearing metadata from each document to avoid showing it\n",
        "for doc in docs_split:\n",
        "    doc.metadata = {}  # Or setting to None, using empty dict\n"
      ],
      "metadata": {
        "id": "TtcoZVPc7xlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating Vector Store for Retrieval"
      ],
      "metadata": {
        "id": "CwqzyTvqRFED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing embedding mode, creating FAISS vector store, saving vector index and returning the vector store intance"
      ],
      "metadata": {
        "id": "CQLoLtoz-1Ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "def create_vector_store(docs_split, embedding_model_name=\"all-MiniLM-L6-v2\", save_path=\"vector_db_index\"):\n",
        "    embedding_model = SentenceTransformerEmbeddings(model_name=embedding_model_name)\n",
        "    vector_db = FAISS.from_documents(docs_split, embedding_model)\n",
        "    vector_db.save_local(save_path)\n",
        "    return vector_db\n",
        "\n",
        "vector_db = create_vector_store(docs_split)"
      ],
      "metadata": {
        "id": "kSEZZj7CnzwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the language model"
      ],
      "metadata": {
        "id": "tIH6nbKXRGsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "\n",
        "def load_llm(model_name=\"google/flan-t5-base\", max_new_tokens=256):\n",
        "\n",
        "    # Initializing text-to-text generation pipeline with selected model\n",
        "    hf_pipe = pipeline(\"text2text-generation\", model=model_name, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # Wrapping the pipeline in a LangChain interface for LLMs and returning llm\n",
        "    llm = HuggingFacePipeline(pipeline=hf_pipe)\n",
        "    return llm\n",
        "\n",
        "# Loading the language model for answering questions\n",
        "llm = load_llm()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es7yosmbn3jY",
        "outputId": "dd161014-9f0c-4c4a-f097-dae10af68b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the RAG Chain"
      ],
      "metadata": {
        "id": "u_B_HrB6RJXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "def build_rag_chain(vector_db, llm, k=5):\n",
        "\n",
        "    # Creating retriever to fetch top-k relevant documents from vector store\n",
        "    retriever = vector_db.as_retriever(search_kwargs={\"k\": k})\n",
        "\n",
        "    # Building the RetrievalQA chain with concatenation of retrieved chunks ('stuff')\n",
        "    # Enabling return of source documents for explainability\n",
        "    rag_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    # Returning the RAG chain ready to answer queries\n",
        "    return rag_chain\n",
        "\n",
        "# Constructing the RAG question-answering chain\n",
        "rag_chain = build_rag_chain(vector_db, llm)\n"
      ],
      "metadata": {
        "id": "ggd9uUChn57-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Asking Questions"
      ],
      "metadata": {
        "id": "jiN_EX45RLrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the result that should we get on asking a question."
      ],
      "metadata": {
        "id": "_mPo41VBAhQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_question(question):\n",
        "    prompt = (question)\n",
        "    result = rag_chain.invoke({\"query\": prompt})\n",
        "\n",
        "    # Printing the question entered\n",
        "    print(f\"Question: {question}\\n\")\n",
        "    print(\"Answers: \\n\")\n",
        "\n",
        "    # Spliting the result into sentences and print each on a new line\n",
        "    sentences = result['result'].strip().split('.')\n",
        "    for sentence in sentences:\n",
        "        if sentence.strip():  # Avoid printing empty lines\n",
        "            print(sentence.strip() + \".\")\n",
        "    print(\"\\n\"*50) # Adding spacing after the answer\n",
        "    return result"
      ],
      "metadata": {
        "id": "PBnqmpbUn8eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ask the question based on the attached files"
      ],
      "metadata": {
        "id": "80vwGMbSRMka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask_question(input(\"Enter your question: \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJC0wGEen-9T",
        "outputId": "aba408bc-1067-4cd5-b2ae-d6248b6f442f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: give the summary for LLaMA Open and Efficient Foundation Language Models \n",
            "Question: give the summary for LLaMA Open and Efficient Foundation Language Models \n",
            "\n",
            "Answers: \n",
            "\n",
            "LLaMA: Open and Efficient Foundation Language Models Hugo Touvron, Thibaut Lavril, Gautier Izacard , Xavier Martinet Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin Edouard Grave, Guillaume Lample Meta AI Abstract We introduce LLaMA, a collection of founda- tion language models ranging from 7B to 65B parameters.\n",
            "We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly avail- able datasets exclusively, without resorting to proprietary and inaccessible datasets.\n",
            "We hope that releasing these models to the research community will accelerate the development of large language models, and help efforts to improve their robustness and mitigate known issues such as toxicity and bias.\n",
            "Additionally, we observed like Chung et al.\n",
            "(2022) that finetuning these models on instructions lead to promising results, and we plan to further investigate.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'give the summary for LLaMA Open and Efficient Foundation Language Models ',\n",
              " 'result': 'LLaMA: Open and Efficient Foundation Language Models Hugo Touvron, Thibaut Lavril, Gautier Izacard , Xavier Martinet Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin Edouard Grave, Guillaume Lample Meta AI Abstract We introduce LLaMA, a collection of founda- tion language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly avail- able datasets exclusively, without resorting to proprietary and inaccessible datasets. We hope that releasing these models to the research community will accelerate the development of large language models, and help efforts to improve their robustness and mitigate known issues such as toxicity and bias. Additionally, we observed like Chung et al. (2022) that finetuning these models on instructions lead to promising results, and we plan to further investigate',\n",
              " 'source_documents': [Document(id='8ca49651-d647-486e-847c-31e21f192ebc', metadata={}, page_content='LLaMA: Open and Efﬁcient Foundation Language Models\\nHugo Touvron∗, Thibaut Lavril∗, Gautier Izacard ∗, Xavier Martinet\\nMarie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal\\nEric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin\\nEdouard Grave∗, Guillaume Lample∗\\nMeta AI\\nAbstract\\nWe introduce LLaMA, a collection of founda-\\ntion language models ranging from 7B to 65B\\nparameters. We train our models on trillions\\nof tokens, and show that it is possible to train\\nstate-of-the-art models using publicly avail-\\nable datasets exclusively, without resorting\\nto proprietary and inaccessible datasets. In\\nparticular, LLaMA-13B outperforms GPT-3\\n(175B) on most benchmarks, and LLaMA-\\n65B is competitive with the best models,\\nChinchilla-70B and PaLM-540B. We release\\nall our models to the research community1.\\n1 Introduction\\nLarge Languages Models (LLMs) trained on mas-\\nsive corpora of texts have shown their ability to per-\\nform new tasks from textual instructions or from a'),\n",
              "  Document(id='0500c5fe-c93a-43d5-b2f6-afb9f3e81ece', metadata={}, page_content='notably, LLaMA-13B outperforms GPT-3 while\\nbeing more than 10× smaller, and LLaMA-65B is\\ncompetitive with Chinchilla-70B and PaLM-540B.\\nUnlike previous studies, we show that it is possible\\nto achieve state-of-the-art performance by training\\nexclusively on publicly available data, without\\nresorting to proprietary datasets. We hope that\\nreleasing these models to the research community\\nwill accelerate the development of large language\\nmodels, and help efforts to improve their robust-\\nness and mitigate known issues such as toxicity and\\nbias. Additionally, we observed like Chung et al.\\n(2022) that ﬁnetuning these models on instructions\\nlead to promising results, and we plan to further\\ninvestigate this in future work. Finally, we plan to\\nrelease larger models trained on larger pretraining\\ncorpora in the future, since we have seen a constant\\nimprovement in performance as we were scaling.'),\n",
              "  Document(id='bba79a29-b550-415d-aaff-a17014a88a50', metadata={}, page_content='Kaiser, Mohammad Bavarian, Clemens Winter,\\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\\nWilliam Saunders, Christopher Hesse, Andrew N.\\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\\nMorikawa, Alec Radford, Matthew Knight, Miles\\nBrundage, Mira Murati, Katie Mayer, Peter Welin-\\nder, Bob McGrew, Dario Amodei, Sam McCandlish,\\nIlya Sutskever, and Wojciech Zaremba. 2021. Eval-\\nuating large language models trained on code.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\\nHutchinson, Reiner Pope, James Bradbury, Jacob'),\n",
              "  Document(id='c8decd70-3ebc-4447-8332-83d23dae7515', metadata={}, page_content='selected words. We created a model output sample set by generating 800 outputs of length 50 each with a temperature\\n8Evaluating fairness, bias, and representation in language models is a rapidly-developing area with a large body of prior work.\\nSee, for example, [HZJ+19, NBR20, SCNP19].\\n36'),\n",
              "  Document(id='2559f286-7aa5-4c3b-9890-cefd77f26633', metadata={}, page_content='We evaluate the ability of our models to write\\ncode from a natural language description on two\\nbenchmarks: HumanEval (Chen et al., 2021) and\\nMBPP (Austin et al., 2021). For both tasks, the\\nmodel receives a description of the program in a\\nfew sentences, as well as a few input-output ex-\\namples. In HumanEval, it also receives a function\\nsignature, and the prompt is formatted as natural\\ncode with the textual description and tests in a')]}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    }
  ]
}